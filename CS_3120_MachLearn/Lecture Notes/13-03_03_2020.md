# <div style="text-align: right">Machine Learning - March 3, 2020</div>
### Agenda:
- Review Homework 2
- Examine Homework 2 Code options
- Image Classification
### Homework:
- Assignment 3 could be posted today or sometime this week
---
## I. Multiclass Classification
- Whereas binary classifiers distinguish between two classes, *multiclass classifiers* (also called multinomial classifiers) can distinguish between more than two classes.
- Some algorithms (such as Random Forest Classifiers or naive Bayes Classifiers) are capable of handling multiple classes directly.
- Others (such as Support Vector Machine classifiers or Linear classifiers) are strictly binary classifiers
- However, there are various strategies that you can use to perform multiclass classification using multiple binary classifiers.
- For example
    - One way to create a system that can classify the digit images into 10 classes (from 0 to 9) is to train 10 binary classifiers, one ofr each digit (a 0-detector, a 1-detector, a 2-detector, and so on).
    - Then when you want to classify an image, you get the decision score from each classifier for that image and you select the class whose classifier outputs the highest score.
    - This is called the `one-versus-all` (OvA) strategy (also called `one-versus-the-rest`)
- Another strategy to train a binary classifier for every pair of digits: one to distinguish 0s and 1s, another to distinguish 0s and 1s and so on.
    - This is called *one-versus-the-rest (OvO) strategy. 
- If there are N classes, you need to train N * (N - 1)/ 2 classifiers.
- For the MNIST problem. this means training 45 binary classifiers!
- When you want to classify an image, you have to run the image through all 45 classifiers and see which class wins the most duels.
- The main advantage of OvO is that each classifier only needs to be trained on the part of the training set for the two classes that it must distinguish.
- Some Algorithms (such as Support Vector Machine Classifiers) scale poorly with the size of the training set, so for these algorithms OvO is preferred since it is faster to train many classifiers on small training sets than training few classifiers on large training sets.
- For most binary classificiation algorithms, however OvA is preferred.
- Scikit-Learn detects when you try to use a binary classification algorithm for a multiclass classification task, and it automatically runs OvA (except for SVM classifiers for which it uses OvO).
- Example:
```python 
sgd_clf.fit(Xtrain, y_train)
sgd_clf.predict([some_digit])

#output
array([5.])
```
- This code trains the SGDClassifier on the training set using the original target classes from 0 to 9 (y_train), instead of the 5-versus-all target classes (y_train_5). 
- Then it makes a prediction (a correct one in this case).
- Under the hood, Scikit-learn actually trained 10 binary classifiers, got their decision scores for the image, and selected the class with the highest score.
- To see that this is indeed the case, you can call the `decision_function()` method.
- Instead of returning just one score per instance, it now returns 10 scores, one per class.
```python 
some_digit_scores = sgd_clf.deision_function([some_digit])
some_digit_scores

# output
array([[-311402.62954431, -363517.28355739, -446449.5306454 ,
-183226.61023518, -414337.15339485, 161855.74572176,
-452576.39616343, -471957.14962573, -518542.33997148,
-536774.63961222]])
```
- The highest score is indeed the one corresponding to class 5:
```python
>>> np.argmax(some_digit_scores)
5
>>> sgd_clf.classes_
array([ 0., 1., 2., 3., 4., 5.,
>>> sgd_clf.classes[5]
5.0
6.,
7.,
8.,
9.])
```
- When a classifier is trained, it stores the list of target classes in its `classes_` attribute, ordered by value.
- In this case, the index of each class in the `classes_` array conveniently matches the class itself (e.g., the class at index 5 hapens to be class 5), but in general you won't be so lucky.
- If you want to force SckitLearn to use one-versus-one or one-versus-all, you can use the `OneVsOneClassifier` classes.
- Simply create an instance and pass a binary classifier to its contructor.
- For Example, this code creates a multiclass classifier using the OvO strategy, based on SGDClassifier:
```python 
>>> from sklearn.multiclass import OneVsOneClassifier
>>> ovo_clf = OneVsOneClassifier(SGDClassifier(random_state=42))
>>> ovo_clf.fit(X_train, y_train)
>>> ovo_clf.predict([some_digit])
array([ 5.])
>>> len(ovo_clf.estimators_)
45
```
- Training a `RandomForestClassifier` is just as easy:
```python 
>>> forest_clf.fit(X_train, y_train)
>>> forest_clf.predict([some_digit])
array([ 5.])
```
- This time Scikit-Learn did not have to run OvA or OvO because Random Forest classifiers can directly classify instances into multiple classes. 
- You can call `predict_prba()` to get the list of probabilities that the classifier assigned to each instance for each class:
```python 
>>> forest_clf.predict_proba([some_digit])
array([[ 0.1, 0. , 0. , 0.1, 0. , 0.8, 0. , 0. , 0. , 0. ]])
```
- You can see that the classifier is fairly confident about its prediction: the 0.8 at the 5th index in the array means that the model estimates an 80% probability that the image represents a 5.
- It also thinks that the image could instead be a 0 or a 3 (10% chance each)
- Now of course you want to evaluate these classifiers. 
- As usual, you want to use cross-validation.
- Lets's evaluate the `SGDClassifier's` accuracy using the `cross_val_score()` function:
```python
>>> cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring="accuracy")
array([ 0.84063187, 0.84899245, 0.86652998])
```
- It gets over 84% on all test folds.
- If you used a random classifier, you would get 10% accuracy, so this is not such a bad score, but yo ucan still do much better.
- For example, simply scaling the inputs (as discussed in Chapter 2) increases accuracy above 90%:
```python
>>> from sklearn.preprocessing import StandardScaler
>>> scaler = StandardScaler()
>>> X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))
>>> cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring="accuracy")
array([ 0.91011798, 0.90874544, 0.906636 ])
```
---
