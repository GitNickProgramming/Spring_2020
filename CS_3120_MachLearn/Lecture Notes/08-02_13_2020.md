# <div style="text-align: right"> February 13, 2020</div>
### Agenda:
- Model Fitting
- More about Gradient Descent
- Logistic Regression
### Homework:
- 
---
## I. Model Fitting
- A more complex model does not always lead to better performance on `testing data`
- This is `Overfitting` --> select suitable model
- Simplier Model:
    - Higher Bias
    - Lower Variance
- Complex Model:
    - Lower Bias
    - Higher Variance
- Diagnosis:
    - If your model cannot even fit the training examples, then you have large bias (underfitting).
    - If you can fit the training data, but large error on testing data, then you probably have large variance (overfitting).
- For Bias, redesign your model:
    - Add more features as input
    - A complex model
- What to do with large Variance?
    - More data, very effective but not always practical
    - Regularization
## II. More about the Gradient Descent
- Stochastic Gradient Descent (also incremental gradient descent)
```
        Loop {
            for i = 1 to m, {
                theta(j) := theta(j) + alpha(y^(i) - h(theta)(x^(i))x(j)^(i)    (for every j)
            }
        }
```
```     Repeat Until Convergence{
                Theta(j) := Theta(j) + alpha(Summation) (y^(i) - h(theta)(x^(i))) x(j)^(i)      (for every j)
        }
```
- Repeatedly run through the training set, and `each time` we encounter one training example, we update the parameters according to the gradient of the error with respect to that single training example only.
- Often, stochastic gradient descent gets theta "close" to the minimum much faster than batch gradient descent. (Howver that it may never "converge" to the minimum, but a super small value is usally good enough.)
- For these reasons, particularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent. 
## III. Logistc Regression
- Classification:
    - Like regression, classification is also a supervised learning algorithm capable of building a modle to predict a target variable based on the input independent variables
    - Unlike regression, in classification, target variable is a categorical variable.
        - The target variable is not a continuous real number.
        - `BloodType` variable takes any value from {A, AB, B, O}
        - State variable from a person's address is also categorical - one of the 50 states.
    - Clasification can be a 2-class or a multi-class problem
    - 2-class classification (i.e., Binary Classification)
        - One of the classes is called "negative class" (0), and the other is called "positive class" (1)
        - `Email`: spam/ not spam?
        - `Tumor`: Malignant / Benign?
        - `Transaction`: Fraudulent yes/no?
    - Multi-class Classification:
        - `Shape`: circle/triangle/rectangle?
        - `Fruit`: Orange/Banana/Apple/Pears/Grape?
- Can We apply Linear regression to do classification?
    - If we do linear regression o nthe binary class target "Tumor", and set a threshold value of h(w)(x):
        - If H(w)(X) >= .05, predict "yes", i.e., class 1, or "positive"
        - Else, predict "not malignant", "class 0", or "negative"
    - But, setting up the "good" threshold might be a nightmare!
        - Take a look at the change of thresholds after adding new samples into the training set.
        - Even, if the target is transformed from categorical "yes", "no" to 1 and 0, the predicted regression values h(w)(X) can be greater than 1 or less than 0
        - h(w)(x) > 1, or h(w)(x) < (using linear regression)
        - Logistic regression guarantees that 0 <= h(w)(x) <= 1 so that you can set a good and stable threshold to do the classification.
- Interpretation of the Hypothesis Output
    - h(w)(X) is estimated as p(y = 1| x, w)
    - 
