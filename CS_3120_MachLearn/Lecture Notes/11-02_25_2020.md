# <div style="text-align: right">Machine Learning - February 25, 2020</div>

### Agenda:
- Multi-class Classification
- Pandas
### Homework:
- Assignement 2 Assigned (due next tuesday)
---
## I. Multi-class Classification
- Whereas binary classifiers distinguish between two classes, *multiclass classifiers* (also called multinomial classifiers) can distinguish between more than two classes
- Some algorithms (such as Random Forest classifiers or naive Bayes Classifiers) are capable of handling multiple classes directly
- For Example:
    - one way to create a system that can classify the digit images into 10 classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-detector, and so on).
    - Then when you want to classify an image, you get the decision score from each classifier for that image and you select the class whose classifier outs the highest score.
    - This is called the *one-versus-all* (OvA) strategy (also called *one-versus-the-rest*).
---
## II. Confusion Matrix
- It's often more convenient to look at an image representation of the confusion matrix, using Matplotlib's *matshow()* function:
```python
        plt.matshow(conf_mx, cmap=plt.cm.gray)
        plt.show()
```
- Analyzing the confusion matrix can often give you insights on ways to improve your classifier.
- Analyzing individual erros can also be a good way to gain insights on what your classifier is doing and why it is failing, but it is more difficult and time-consuming.
---
## III. Multilabel Classification
- Until now each instance has always been assigned to just one class.
- In some cases you may want your classifier to output multiple classes for each instance. 
- For Example:
    - Consider a face-recognition classifier:
        - What should it do if it recognizes several people on the same picture?
        - Of course it should attach one label per person it recognizes. 
        - Say the classifier has been trained to recognize three faces, Alice, Bob, and Charlie;
        - Then when it is show a picture of Alice and Charlie, it should output [1, 0, 1] (meaning "Alice yes, Bob no, Charlie yess"). 
        - Such a classification system that outputs multiple binary labels is called a *multilabel classification* system.
```python 
from sklearn.neighbors import KNeighborsClassifier

y_train_large = (y_train >= 7)
y_train_odd = (y_train % 2 == 1)
y_multilabel = np.c_[y_train_large, y_train_odd]

knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_multilabel)
```
- This code creates a `y_multilabel` arry containing two target labels for each digit image: the first indicates whether or not the digits is large (7, 8, or 9) and the second indicates whether or not it is odd.
- The next lines crate a `KNeighborsClassifier` instance (which supports multilabel classification, but not all classifiers do) and we train it using the multiple targets array. 
- Now you can make a prediction
```python
    knn_clf.predict([some_digit])
```
- There are many ways to evaluate a multilabel classifier, and selecting the right metric really depends on your project.
- For example:
    - one approach is to measure the F1 score across all labels:
```python
        y_train_knn_pred = cross_val_predict(knn_clf, X_train, cv = 3)
        f1_score(y_train, y_train_knn_pred, average="macro")
```
- Output = 0.96845549180221
- This assumes that all labels are equally important, which may not be the case.
- In particular, if you have many more pictures of Alice than of Bob or Charlie, you may want to give more weight to the classifier's score on pictures of Alice.
- One simple option is to give each label a weight equal to its *support*
    - I.E., the number of instances with that target label. 
    - To do this, simply set `average = "weighted"` in the preceding code
---
## IV. Multioutput Classification
- The last type of classification task we are going to discuss here is called *multioutput multiclass classification*
    - Or simply multioutput classification
- It is simply a generalization of multilabel classification where each label can be multiclass.
    - It can have more than two possible values
- The line between classification and regression is sometimes blurry, such as in this example.
    - Arguably, predicting pixel intensity is more akin to regression than to classification tasks;
    - Moverover, multioutput systems are not limited to classification tasks
    - You can even have a system that outputs multiple labels per instance, including both class labels and value labels
    - *Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for more details*
---
## V. Pandas
- A python package providing fast, flexible, and expresive data structures designed to make working with "relational" or "labeled" data both easy and intuitive.
- It aims to be the fundamental high-level building block of doing practical, real world data analysis in Python.
- Panas is well suited for many different kinds of data:
    - Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet
    - Ordered and unordered (not necesarily fixed-frequency) time series data.
    - Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels
    - Any other form of obersvational / statistical data sets.
    - The data actually need not be labeled at all to be placed into a pandas data structure.
- Here are just a few of the things that pandas does well:
    1. Easy handling of **missing data** (represented as NaN) in floating point as well as non-floating point data.
    2. Size Mutablity:
        - columns can be **inserted and deleted** from DataFrame and higher dimensional objects
    3. Automatic and explicit **data alignment**: objects can be explicitly aligned to a set of labels, or the user can simply ignore the labels and let *Series, dataFrame, etc* automatically align the data for you in computations.
    4. Make it **easy to convert** ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects.
    5. Intelligent label-based **slicing, fancy indexing,** and **subsetting** of large data sets
    6. Intuitive **merging** and **joining** data sets
    7. Flexible **reshaping** and privoting of data sets
    8. **Hierarchical** labeling of axes (possible to have multiple labels per tick)
    9. Robust IO tools for loading data from **flat files** (CSV and delimited), Excel files, databases, and saving / loading data from the ultrafast **HDF5 format**
    10. **Time series** specific functionality:
        - Date range generation and frequency conversion, moving window statistics, date shifting and lagging.
- Many of these principles are here to address the shortcomings frequently experienced using other languages / scientific research environments.
- For data scientists, working with data is typically divided into multiple stages:
    - Munging and cleaning data
    - Analyzing / modeling it
    - Organizing the results of the analysis into a form suitable for plotting or taular display.
