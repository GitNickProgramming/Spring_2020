 
# <div style="text-align: right">Machine Learning - February 20, 2020</div>

### Agenda:
- 
### Homework:
- 
---
## I. Logistic Regression:
- If the estimated probability is greater than 50%, then the model predicts that the instance belongs to that class (called the positive class, labeled "1"), or else it predicts that it does not (i.e., it belongs to the negative class, labeled "0").
- This makes it a binary 
---
## II. Estimating Probabilities
- Just like Linear Regression Model, a Logistic Regression model computes a weighted sum of the input features (plus a bias term), but instead of outputting the result directly like the Linear Regression Model does, it outputs the logistic of this result.
- Equation 4-13. Logistic Regression model estimated probability (vectorized form)

    $$\hat{p} = h_\theta(x) = \delta(\theta^T * x)$$
    
- The logistic - also called the *logit*, noted $\sigma(*)$ is a *sigmoid function* (i.e., S-Shaped) that outputs a number between 0 and 1.
- It is defined as shown in **Equation 4-14**

$$\frac{1}{1 + exp(-t)}$$
- Once the Logistic Regression Model has estimated the probability $\hat{p} = h_\theta(x)$ that an instance x belongs to the postive class, it can make its prediction $\hat{y}$ easily (see **Equation 4-15**)

$$\hat{y} = \begin{cases}
0, & \text{if } \hat{p} < 0.5, \\
1, & \text{if } \hat{p} \geq 0.5 \end{cases}$$

- Notice that $\sigma(t) \le 0.5$ when $t < 0$, and $\sigma(t) \geq 0.5$ when $t \geq 0$, so a Logistic Regression model predicts 1 if $\theta^T * x$ is positive, and 0 if it is negative.
---
## III. Training and Cost Function
- Good, now you know how a logistic Regression model estimates probabilities and makes predictions.
- But how is it Trained?
    - The objective of training is to set the parameter vector $\theta$ so that the model estimates high probabilities for positive instances (y = 1) and low probabilities for negative instances (y = 0). 
    - This idea is captured by the cost function shown in in **Equation 4-16** for a single training instances **x**

```
            Equation 4-16. Cost Function of a single training instance
```

$$c(\theta)=\begin{cases}
-log(\hat{p}), & \text{if } y = 1, \\
-log(1 - \hat{p}), & \text{if } y = 0 \end{cases}$$

- This cost function makes sense because $-log(t)$ grows very large when $t$ approaches 0, so the cost will be large if the model estimates a probability close to 0 for positive instance, and it will also be very large if the model estimates a probability close to 1 for a negative instance.
- On the other hand, $-log(t)$ is close to 0 when $t$ is close to 1, so the cost will be closest to 0 if the estimated probability is close to 0 for a negative instance or closest to 1 for a positive instance, which is precisely what we want. 
- The cost function over the whole training set is simply the average cost over all training instances.
- It can be written in a single expression (as you can verify easily), called the $log$ $loss$, show in **Equation 4-17**

$$J(\theta) =-\frac{1}{m}\sum_{i=1}^m [y^{(i)} log(\hat{p}^{(i)}) + (1 - y^{(i)}) log(1 - \hat{p}^{(i)})]$$

- The bad news is that there is no know closed-form equation to compute the value of \theta that minimizes this cost function (there is no equivalent of the Normal Equation). 
- But the good news is that this cost function is convex, so Gradient Descent (or any other optimization algorithm) is guaranteed to find the global minimum (if the learning rate is not too large and you wait long enough).
- The Partial derivatives of the cost function with regards to the $j^{th}$ model parameter \theta_j is given by **Equation 4-18**
```
Equation 4-18. Logistic cost function partial derivatives
```